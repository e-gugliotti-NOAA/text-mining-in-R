---
title: "text-mining-in-R"
output: html_document
date: "2023-12-15"
---

```{r}
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(gutenbergr)
library(tidyr)
library(scales)
library(textdata)
library(wordcloud)
library(reshape2)
library(forcats)
library(ggraph)
library(widyr)
```

# Tidy text
## Jane Austen Books
```{r}
original_books <- austen_books()|>
  group_by(book) |>
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text,
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) |>
  ungroup()

original_books


# Put in one word per row format
# Token = word here but can also be characters, n-grams, paragraphs, etc.
tidy_books <- original_books |>
  unnest_tokens(word, text)

tidy_books


# Remove stop words such as the, to, of
data(stop_words)
tidy_books <- tidy_books |> 
  anti_join(stop_words)

tidy_books |> 
  count(word, sort = TRUE) |> 
  filter(n > 600) |> 
  mutate(word = reorder(word, n)) |> 
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL)
```


## Project Gutenberg
```{r}
# Word frequencies
# Gutenberg ids for HG Wells books = 35, 36, 5230, 159

hgwells <- gutenberg_download(c(35,36,5230,159))

tidy_hgwells <- hgwells |> 
  unnest_tokens(word, text) |> 
  anti_join(stop_words)

tidy_hgwells |> 
  count(word, sort = TRUE)

# Get Bronte sister books
bronte <- gutenberg_download(c(1260, 768, 969, 9182))

tidy_bronte <- bronte |> 
  unnest_tokens(word, text) |> 
  anti_join(stop_words)

tidy_bronte |> 
  count(word, sort = TRUE)
```

## Calc frequency of words in all 3 authors works and compare word frequencies of Jane Austen, the Brontë sisters, and H.G. Wells

Words that are close to the line in these plots have similar frequencies in both sets of texts.
```{r}
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"),
                       mutate(tidy_books, author = "Jane Austen")) |> 
  mutate(word = str_extract(word, "[a-z']+")) |> 
  count(author, word) |> 
  group_by(author) |> 
  mutate(proportion = n/sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = author, values_from = proportion) |> 
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

ggplot(frequency, aes(x = proportion, y = `Jane Austen`,
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001),
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Jane Austen", x = NULL)

cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~proportion + `Jane Austen`)

cor.test(data = frequency[frequency$author == "H.G. Wells",],
         ~proportion + `Jane Austen`)
```

# Sentiment analysis
## Inner join
```{r}
nrc_joy <- get_sentiments("nrc") |> 
  filter(sentiment == "joy")

tidy_books |> 
  filter(book == "Emma") |> 
  inner_join(nrc_joy) |> 
  count(word, sort = TRUE)

jane_austen_sentiment <- tidy_books |> 
  inner_join(get_sentiments("bing")) |> 
  count(book, index = linenumber %/% 80, sentiment) |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive-negative)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

pride_prejudice <- tidy_books |> 
  filter(book == "Pride & Prejudice")

afinn <- pride_prejudice |> 
  inner_join(get_sentiments("afinn")) |> 
  group_by(index = linenumber %/% 80) |> 
  summarize(sentiment = sum(value)) |> 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice |> 
    inner_join(get_sentiments("bing")) |> 
    mutate(method = "Bing et al."),
  pride_prejudice |> 
    inner_join(get_sentiments("nrc") |> 
                 filter(sentiment %in% c("positive","negative"))) |> 
    mutate(method = "NRC")) |> 
  count(method, index = linenumber %/% 80, sentiment) |> 
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) |> 
  mutate(sentiment = positive - negative)

bind_rows(afinn, bing_and_nrc) |> 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

## Most common positive and negative words
```{r}
bing_word_counts <- tidy_books |> 
  inner_join(get_sentiments("bing")) |> 
  count(word, sentiment, sort = TRUE) |> 
  ungroup()

bing_word_counts |> 
  group_by(sentiment) |> 
  slice_max(n, n = 10) |> 
  ungroup() |> 
  mutate(word = reorder(word, n)) |> 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment", y = NULL)

custom_stop_words <- bind_rows(tibble(word = c(miss),
                                      lexicon = c("custom")),
                               stop_words)
```

## Wordclouds
```{r}
tidy_books |> 
  anti_join(stop_words) |> 
  count(word) |> 
  with(wordcloud(word, n, max.words = 100))

tidy_books |> 
  inner_join(get_sentiments("bing")) |> 
  count(word, sentiment, sort = TRUE) |> 
  acast(word~sentiment, value.var = "n", fill = 0) |> 
  comparison.cloud(colors = c("gray20", "gray80"), max.words = 100)
```

# Sentiment analysis algorithms
```{r}
# Split by sentence
p_and_p_sentences <- tibble(text = prideprejudice) |> 
  unnest_tokens(sentence, text, token = "sentences")

p_and_p_sentences$sentence[5]

# Split by chapter
austen_chapters <- austen_books() |> 
  group_by(book) |> 
  unnest_tokens(chapter, text, token = "regex", pattern = "Chapter|CHAPTER [\\dIVXLC]") |> 
  ungroup()

austen_chapters |> 
  group_by(book) |> 
  summarise(chapters = n())

# What are the most negative chapters in each of Jane Austen's novels?
bingnegative <- get_sentiments("bing") |> 
  filter(sentiment == "negative")

wordcounts <- tidy_books |> 
  group_by(book, chapter) |> 
  summarize(words = n())

tidy_books |> 
  semi_join(bingnegative) |> 
  group_by(book, chapter) |> 
  summarize(negativewords = n()) |> 
  left_join(wordcounts, by = c("book","chapter")) |> 
  mutate(ratio = negativewords/words) |> 
  filter(chapter != 0) |> 
  slice_max(ratio, n = 1) |> 
  ungroup()
```

# Analyzing word and document frequency with tf-idf
## Term frequency in Jane Austen's novels
```{r}
book_words <- austen_books() |> 
  unnest_tokens(word, text) |> 
  count(book, word, sort = TRUE)

total_words <- book_words |> 
  group_by(book) |> 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

## Zipf's Law, term-frequency and rank
```{r}
freq_by_rank <- book_words |> 
  group_by(book) |> 
  mutate(rank = row_number(),
         `term frequency` = n/total) |> 
  ungroup()

freq_by_rank

# Zipf's law is visualized by plotting rank on the x-axis and term-frequency on 
# the y axis, on logarithmic scales
freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = book)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

# See what the exponent of the power law is for the middle section of the rank range
rank_subset <- freq_by_rank |> 
  filter(rank < 500,
         rank > 10)

lm_power <- lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = book)) +
  geom_abline(intercept = lm_power$coefficients[1], slope = lm_power$coefficients[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

## `bind_tf_idf()` function
The idea of tf-idf is to find important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents (in this case, the group of Jane Austen novels as a whole). Calculating tf-idf attempts to find the words that are important in a text, but not too common.

The tf-idf will be zero for extremely common words.
```{r}
book_tf_idf <- book_words |> 
  bind_tf_idf(word, book, n)

book_tf_idf

book_tf_idf |> 
  select(-total) |> 
  arrange(desc(tf_idf))

book_tf_idf |> 
  group_by(book) |> 
  slice_max(tf_idf, n = 15) |> 
  ungroup() |> 
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

# Relationships between words: n-grams and correlations
## Tokenizing by n-gram
```{r}
austen_bigrams <- austen_books() |> 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |> 
  filter(!is.na(bigram))

austen_bigrams

austen_bigrams |> 
  count(bigram, sort = TRUE)

bigrams_separated <- austen_bigrams |> 
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated |> 
  filter(!word1 %in% stop_words$word) |> 
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered |> 
  count(word1, word2, sort = TRUE)

bigram_counts

bigrams_united <- bigrams_filtered |> 
  unite(bigram, word1, word2, sep = " ")

bigrams_united

austen_books() |> 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) |> 
  filter(!is.na(trigram)) |> 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") |> 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) |> 
  count(word1, word2, word3, sort = TRUE)

bigrams_filtered |> 
  filter(word2 == "street") |> 
  count(book, word1, sort = TRUE)

# A bigram can also be treated as a term in a document using tf_idf
bigram_tf_idf <- bigrams_united |> 
  count(book, bigram) |> 
  bind_tf_idf(bigram, book, n) |> 
  arrange(desc(tf_idf))

bigram_tf_idf

bigram_tf_idf |> 
  group_by(book) |> 
  slice_max(tf_idf, n = 15) |> 
  ungroup() |> 
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

# Sentiment analysis of bigrams
bigrams_separated |> 
  filter(word1 == "not") |> 
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

not_words <- bigrams_separated |> 
  filter(word1 == "not") |> 
  inner_join(AFINN, by = c(word2 = "word")) |> 
  count(word2, value, sort = TRUE)

not_words

not_words |> 
  mutate(contribution = n*value) |> 
  arrange(desc(abs(contribution))) |> 
  head(20) |> 
  mutate(word2 = reorder(word2, contribution)) |> 
  ggplot(aes(n*value, word2, fill = n*value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number off occurrences",
       y = "Words preceeded by \"not\"")

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated |> 
  filter(word1 %in% negation_words) |> 
  inner_join(AFINN, by = c(word2 = "word")) |> 
  count(word1, word2, value, sort = TRUE)

negated_words |> 
  mutate(contribution = n * value,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) |> 
  group_by(word1) |> 
  slice_max(abs(contribution), n = 12, with_ties = FALSE) |> 
  ggplot(aes(word2, contribution, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation term") +
  ylab("Sentiment value * # of occurrences") +
  coord_flip()
```

